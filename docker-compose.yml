services:
  # Ollama - Local LLM inference with GPU support
  ollama:
    image: ollama/ollama:0.5.4
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_NUM_GPU=1
      - OLLAMA_GPU_LAYERS=999
      - OLLAMA_NUM_THREAD=16
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_PARALLEL=4
    runtime: nvidia
    restart: unless-stopped
    healthcheck:
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/11434' || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Qdrant - Vector database for RAG (optimized configuration)
  qdrant:
    image: qdrant/qdrant:v1.12.4
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      # gRPC port for high-performance client connections
      - QDRANT__SERVICE__GRPC_PORT=6334
      # Performance tuning: max concurrent searches
      - QDRANT__SERVICE__MAX_SEARCH_THREADS=4
      # Storage optimization: enable mmap for large collections
      - QDRANT__STORAGE__MMAP_ADVICE=random
      # HNSW index optimization: parallel index building
      - QDRANT__STORAGE__OPTIMIZERS__INDEXING_THRESHOLD=20000
      # Memory management: limit segment size for faster updates
      - QDRANT__STORAGE__OPTIMIZERS__MAX_SEGMENT_SIZE=200000
      # Enable WAL for durability with good performance
      - QDRANT__STORAGE__WAL__WAL_CAPACITY_MB=64
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
    restart: unless-stopped
    healthcheck:
      test: timeout 10s bash -c ':> /dev/tcp/127.0.0.1/6333' || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # Redis - Conversation memory and caching
  redis:
    image: redis:7.4-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru --maxclients 1000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Backend API - FastAPI with RAG pipeline
  backend:
    image: jconover/ai-rag-backend:latest
    container_name: rag-backend
    ports:
      - "8000:8000"
    volumes:
      - ./data:/data
      - ./scripts:/scripts
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - PYTHONUNBUFFERED=1
      - LOG_LEVEL=info
      - ENABLE_PROMETHEUS_METRICS=true
    depends_on:
      ollama:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      redis:
        condition: service_healthy
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Prometheus - Metrics collection
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=7d'
    depends_on:
      - backend
    restart: unless-stopped

  # Grafana - Metrics visualization
  grafana:
    image: grafana/grafana:10.2.0
    container_name: grafana
    ports:
      - "3001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    depends_on:
      - prometheus
    restart: unless-stopped

  # Frontend - React web UI
  frontend:
    image: jconover/ai-rag-frontend:latest
    container_name: rag-frontend
    ports:
      - "3000:80"
    depends_on:
      - backend
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  ollama_data:
    driver: local
  qdrant_data:
    driver: local
  redis_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  default:
    name: rag-network
