---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama
  namespace: ai-rag-stack
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: llm
    app.kubernetes.io/part-of: ai-rag-stack
    app.kubernetes.io/version: "0.5.4"
  annotations:
    description: "Ollama - Local LLM inference with GPU support"
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: ollama
      app.kubernetes.io/component: llm
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ollama
        app.kubernetes.io/component: llm
        app.kubernetes.io/part-of: ai-rag-stack
    spec:
      terminationGracePeriodSeconds: 60

      containers:
        - name: ollama
          image: ollama/ollama:0.5.4
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 11434
              protocol: TCP

          env:
            - name: OLLAMA_NUM_GPU
              value: "1"
            - name: OLLAMA_GPU_LAYERS
              value: "999"
            - name: OLLAMA_NUM_THREAD
              value: "16"
            - name: OLLAMA_MAX_LOADED_MODELS
              value: "2"
            - name: OLLAMA_KEEP_ALIVE
              value: "24h"
            - name: OLLAMA_NUM_PARALLEL
              value: "4"

          resources:
            requests:
              cpu: "2"
              memory: "8Gi"
              nvidia.com/gpu: "1"
            limits:
              cpu: "8"
              memory: "16Gi"
              nvidia.com/gpu: "1"

          livenessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 60
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3

          readinessProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3

          startupProbe:
            httpGet:
              path: /
              port: http
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 30

          volumeMounts:
            - name: ollama-data
              mountPath: /root/.ollama

      volumes:
        - name: ollama-data
          persistentVolumeClaim:
            claimName: ollama-data

      # Node selector for GPU nodes
      nodeSelector:
        nvidia.com/gpu.present: "true"

      tolerations:
        - key: nvidia.com/gpu
          operator: Exists
          effect: NoSchedule
---
apiVersion: v1
kind: Service
metadata:
  name: ollama
  namespace: ai-rag-stack
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: llm
    app.kubernetes.io/part-of: ai-rag-stack
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 11434
      targetPort: http
      protocol: TCP
  selector:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: llm
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-data
  namespace: ai-rag-stack
  labels:
    app.kubernetes.io/name: ollama
    app.kubernetes.io/component: storage
    app.kubernetes.io/part-of: ai-rag-stack
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
  # Uncomment and set your storage class
  # storageClassName: standard
